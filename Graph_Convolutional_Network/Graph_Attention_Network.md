# GRAPH ATTENTION NETWORKS

## Abstract
1. 本文提出了一种新式的处理图结构数据的神经网络，它利用了自关注层来解决以往图卷积的缺点。
2. 通过堆叠关注层，可以隐式为一个邻域中不同节点赋予不同权重，而不需要高昂的计算量、也不需要预先知道图的结构。
3. 同时本文还解决了谱域卷积的几个关键问题，能使模型处理inductive和transductive问题。
4. 在四个数据集上取得了sota的效果，数据集包括：Cora，Citeseer，Pubmed和Protein-protein interaction。

## Introduction
1. 最近CNN被广泛应用，它所处理的数据是网格结构的，这样的结构能有效重用卷积核。然而很多任务不能被表示为网格结构，它们能表示为不规则域。例如：社交网络、生物网络等，这些通常被表达为图的形式。

2. 目前已有很多尝试将神经网络扩展到处理任意结构的图：
    - RvNN将图域中的数据表示为有向无环图。
    - GNN可以处理更一般的图类，如循环图、有向图和无向图
    - GGNN是在GNN中加入门结构
  
3. 将卷积推广到图领域是有益的，目前主流方法分类两大类：谱域卷积、非谱域卷积。

4. 谱域卷积：
    - 2014年，对图拉普拉斯算子进行特征分解，来定义傅里叶变换域的卷积操作。带来巨大的计算量。
    - 2015年，引入了光滑系数谱滤波器的参数化，使其具有空间局部化特性。
    - 2016年，提出了用拉普拉斯图的切比雪夫展开来近似滤波器，消除了计算拉普拉斯特征向量的需要，并给出了空间局域滤波器。
    - 2017年，通过限制卷积核只在每个节点的一阶邻域进行操作，简化了方法。
    - 结论：上述方法需要图的拉普拉斯算子，是基于输入图结构的，不能接受不同结构的输入图。

## GAT Architecture

## Evaluation

## Conclusion

