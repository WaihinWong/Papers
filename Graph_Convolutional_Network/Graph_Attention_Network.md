# GRAPH ATTENTION NETWORKS

## Abstract
1. 本文提出了一种新式的处理图结构数据的神经网络，它利用了自关注层来解决以往图卷积的缺点。
2. 通过堆叠关注层，可以隐式为一个邻域中不同节点赋予不同权重，而不需要高昂的计算量、也不需要预先知道图的结构。
3. 同时本文还解决了谱域卷积的几个关键问题，能使模型处理inductive和transductive问题。
4. 在四个数据集上取得了sota的效果，数据集包括：Cora，Citeseer，Pubmed和Protein-protein interaction。

## Introduction
1. 最近CNN被广泛应用，它所处理的数据是网格结构的，这样的结构能有效重用卷积核。然而很多任务不能被表示为网格结构，它们能表示为不规则域。例如：社交网络、生物网络等，这些通常被表达为图的形式。

2. 目前已有很多尝试将神经网络扩展到处理任意结构的图：
    - RvNN将图域中的数据表示为有向无环图。
    - GNN可以处理更一般的图类，如循环图、有向图和无向图
    - GGNN是在GNN中加入门结构
  
3. 将卷积推广到图领域是有益的，目前主流方法分类两大类：谱域卷积、非谱域卷积。

4. 谱域卷积：
    - 2014年，对图拉普拉斯算子进行特征分解，来定义傅里叶变换域的卷积操作。带来巨大的计算量。
    - 2015年，引入了光滑系数谱滤波器的参数化，使其具有空间局部化特性。
    - 2016年，提出了用拉普拉斯图的切比雪夫展开来近似滤波器，消除了计算拉普拉斯特征向量的需要。
    - 2017年，通过限制卷积核只在每个节点的一阶邻域进行操作，简化了方法。
    - 结论：上述方法需要图的拉普拉斯算子，是基于输入图结构的，不能接受不同结构的输入图。

5. 非谱域卷积：非谱域方法的难点是定义一个能在不同大小邻域上计算和保持参数共享的性质。
    - 需要学习每个节点度的特定权值矩阵，利用转换矩阵的幂来定义邻域，同时学习每个输入通道的8个权值和邻域度，或者提取包含固定数量节点的邻域并对其进行归一化
    - MoNet为图提供了一种一般化的CNN结构
    - GraphSAGE以归纳方式计算节点表示的方法。通过对每个节点的一个固定大小的邻域进行采样，然后对其执行特定的聚合操作。

6. 本文提出了基于关注的结构对图结构数据的节点进行分类：
    - 它能够通过关注每个节点的邻域来计算每个节点的隐藏表示。
    - 它的关注具有以下性质：
        - 在不同节点对中能够实现并行化。
        - 通过为邻域赋予权重，能应用到具有不同度矩阵。
        - 模型可以直接适用于归纳学习问题，包括可以推广到未知的图。
    - 在四个数据集上取得了sota的效果，数据集包括：Cora，Citeseer，Pubmed和Protein-protein interaction。

## GAT Architecture
提出了基础的关注块层，通过堆叠它可以实现对任意图的处理。并直接概述了其在神经图处理领域的理论和实践优势，和以往工作相比的局限性。



## Evaluation

## Conclusion

